# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/1046 multi-label classifier - export7_noisy_student_weights_clean.ipynb (unless otherwise specified).

__all__ = ['intersect', 'get_meta_subset', 'TorchTransform', 'tfs2', 'calc_class_weights', 'acc_02', 'acc_05',
           'run_pred', 'run_pred_coords', 'plot_pred_tile', 'show_image_tiles', 'show_tiles', 'plot_pred_classes',
           'get_tile_tensors', 'get_tile_images', 'image_list_to_tensors', 'get_vocab', 'run_pred_from_images',
           'run_pred_from_coords', 'show_images_on_grid', 'run_pred_from_df', 'train_cv5_models', 'get_cv5_models',
           'pred_cv5', 'plot_multi_roc', 'plot_merged_preds', 'get_performance', 'plot_multi_prec_recall',
           'plot_cluster_scores', 'show_images_for_predicted_label', 'make_masked_channels', 'plot_seg_maps',
           'img_to_segmaps', 'summarize_image_specs', 'make_segmaps', 'overlay_image_segments', 'plot_contours_holes',
           'add_segment_contours', 'halo_xml', 'prettify', 'xml_build', 'export_halo_xml_from_img']

# Cell
# export

# Note: use fastai 2.2.5 since somehow fastai 2.3.1 lead to library conflicts

import fastai
import pandas as pd
from fastai.distributed import *
from fastai.vision.all import *
import matplotlib.pyplot as plt
import torchvision.transforms as TT
import openslide
import openslide.deepzoom

import plotly.graph_objects as go
import plotly.express as px

import sklearn
from sklearn.metrics import *
from sklearn.model_selection import StratifiedKFold

import wandb
from fastai.callback.wandb import *

# %env WANDB_SILENT=true

from matplotlib.backends.backend_pdf import PdfPages

fastai.__version__

# Cell
def intersect(a, b):
    return list(set(a).intersection(b))

# Cell

# skip this for bioset - done before merging dat_meta

# labels related to biology
# Use qc labels if they are the only tag for the image

def get_meta_subset(dat_meta, to_do_cat, qc_cat, keep_all= True):
    '''
    create subset of labels we want to keep for training

    == input:
    keep_all: keep all rows even if rows are not in the specified categories

    == return:
    a dataframe with columns `Label_bio` as a list of labels and `Label_bio_str` as a `|` separated string

    '''
    dat_meta['Label_bio']= dat_meta['Label'].apply(lambda x: intersect(x.split("|"), to_do_cat))
    dat_meta['Label_bio_str']= dat_meta['Label_bio'].apply(lambda x: "|".join(x))
    dat_meta.loc[dat_meta['Label'].isin(qc_cat), ('Label_bio_str')]= dat_meta.loc[dat_meta['Label'].isin(qc_cat), ('Label')]
    if not keep_all:
        dat_meta= dat_meta[dat_meta['Label_bio_str'] != '']
    return dat_meta

# Cell

class TorchTransform(Transform):
    '''
    wrapper for torchvision transforms
    '''
    def __init__(self, aug): self.aug = aug
    def encodes(self, img: PILImage):
        aug_img = self.aug(img)
        return aug_img

# Cell

# tfs= [RandomResizedCrop(224, min_scale= 0.5)]

tfs2= TT.Compose([
                 TT.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.01, hue=0.01),
                 TT.RandomRotation(45),
                 TT.RandomVerticalFlip(),
                 TT.CenterCrop(size=224)
                 ])



# Cell

# calculate class weights
#  wj=n_samples / (n_classes * n_samplesj)

def calc_class_weights(dls):
    '''
    return class weights based on the inverse method

    '''
    big_y= []
    for i in dls.train_ds:
        big_y.append(np.asarray(i[1]))
        # print(i[1])
    big_y= np.vstack(big_y)
    n_total= np.sum(big_y)
    n_cl_i= np.sum(np.vstack(big_y), axis=0)
    n_classes= big_y.shape[1]

    cw= n_total/ (n_classes*n_cl_i)
    return torch.Tensor(cw)

# Cell

# use different metrics

acc_02 = partial(accuracy_multi, thresh=0.2)
acc_05 = partial(accuracy_multi, thresh=0.5)
# f_score = partial(FBetaMulti, thresh=0.2)
# metrics=[ accuracy_multi ]


# Cell
def run_pred(learn, item, y= None):
    '''
    given a model (learn) and a tensor image (item), plot the input image along with the predicted probabilities

    input:
        learn: learner model
        item: a tensor input image
        y: true labels

    return:
        predicted probabilities
        predicted classes
    '''
    num_lab= len(learn.dls.vocab)

    assert(item.shape[1]== 224)

    # pred= learn.predict(item)
    with torch.no_grad():
        pred= learn.model.eval()(item.unsqueeze(0))
        pred= pred.sigmoid()[0].cpu()

    if y is not None:

        y= learn.dls.vocab[y==1]
        y= "|".join(y)
    fig, axs= plt.subplots(1,2, figsize= (12,6))

    axs[0].imshow(item.cpu().permute(1,2,0))
    axs[0].get_xaxis().set_ticks([])
    axs[0].get_yaxis().set_ticks([])
    axs[0].set_title(y)

    pred_class= learn.dls.vocab[pred > .5]
    axs[1].plot(pred)
    axs[1].set_ylabel('Probability')
    axs[1].set_title("Prediction: {}".format(",".join(pred_class)))
    axs[1].set_xticks(range(num_lab))
    axs[1].set_xticklabels(learn.dls.vocab)
    axs[1].set_ylim(0,1)

    return pred, pred_class

# Cell

def run_pred_coords(learn, dat_zoom, coord):
    '''
    given tile coordinates for a single tile, predict the image class

    coord: tuple of magnification level, x, y
    TO DO: check input dimensions
    '''
    lvl= coord[0]
    tmp_tile= dat_zoom.get_tile(lvl, (coord[1], coord[2]))
    tmp_tile= TT.CenterCrop(224)(tensor(tmp_tile).permute(2,0,1)/255)
    # print(tmp_tile.shape)
    run_pred(learn, tmp_tile.cuda())

# Cell

def plot_pred_tile(preds, nx=10, ny= 10, tight= True, figsize= (12,12)):
    '''
    create a grid plot of prediction scores
    '''

    plt.figure(figsize= figsize)
    for i in range(preds.shape[0]):
        plt.subplot(nx,ny, i+1)
        plt.plot(preds[i])
        # plt.xlabel(xlab)
        plt.ylim(0,1)
        plt.xticks([])
        plt.yticks([])
        if tight:
            plt.axis('off')

    plt.subplots_adjust( wspace=0, hspace=0)

# Cell

def show_image_tiles(coords, dat_zoom, figsize= (12,12)):
    '''
    given a set of coordinates, extract and show the image tiles on a grid
    coords: (level, x, y)
    dat_zoom: openslide deepzoom object
    '''
    n= len(coords)
    nx= 10
    ny= 10
    cnt= 1
    lvl= coords[0][0]

    plt.figure(figsize= figsize)
    for i in coords:
        plt.subplot(nx,ny,cnt)
        plt.imshow(dat_zoom.get_tile(lvl, (i[1], i[2])))
        plt.xticks([])
        plt.yticks([])
        plt.axis('off')
        cnt+= 1

    plt.subplots_adjust( wspace=0, hspace=0)

# backward compability
show_tiles= show_image_tiles

# Cell

def plot_pred_classes(learn, dat_zoom, coords):
    '''
    plot prediction scores of random images on a 10 x 10 grid
    '''
    n= len(coords)
    nx= 10
    ny= 10

    all_img= get_tile_tensors(dat_zoom, coords[0:min(n, nx*ny)])

    with torch.no_grad():
        pred2= learn.model.eval()(all_img.cuda())
        pred2= pred2.sigmoid().cpu()

    return pred2


# Cell

# get images for a batch of tiles
from tqdm import tqdm
import dask
from dask.diagnostics import ProgressBar

def get_tile_tensors(dat_zoom, coords, tfms= TT.CenterCrop(224), progress= True, useDask= True, scale= 1, verbose= False):
    '''
    given tile coordinates, retrieve the image tensors. resize the image tile if scale is not 1

    dat_zoom: a deepzoom object
    coords: a list of tuples of (magnification level, x, y)
    scale: rescale the image tile if not 1
    tfms: image transformation; default is center crop to 224 px
    useDask: use Dask for parallel processing

    '''
    lvl= coords[0][0]
    all_img= []

    # get image tile size from deep zoom
    tile_size= dat_zoom._z_t_downsample

    new_size= None
    if scale!=1:
        new_size= (int(tile_size* scale), int(tile_size* scale))
        if verbose:
            print("original tile size= ", tile_size)
            print("scale= ", scale)
            print("target tile size= ", new_size)

    lazy_results = []

    def get_tile(lvl, x, y, new_size= None):
        '''
        extract tile and resize if new_size is provided
        '''
        if new_size is None:
            tmp_tile= dat_zoom.get_tile(lvl, (x, y))
        else:
            tmp_tile= dat_zoom.get_tile(lvl, (x, y)).resize(new_size)

        tmp_tile= tensor(tmp_tile).permute(2,0,1)/255
        return tmp_tile

    if useDask:
        if progress:
            pbar= ProgressBar()
            pbar.register()
        for i in coords:
            lazy_result = dask.delayed(get_tile)(lvl, i[1], i[2], new_size)
            lazy_results.append(lazy_result)
        lazy_results = dask.compute(*lazy_results)
        # return lazy_results

        if progress:
            pbar.unregister()
        all_img= tensor(np.stack(lazy_results))
    else:
        for i in tqdm(coords, disable= not progress):
            # print(i[1], i[2])
            # tmp_tile= dat_zoom.get_tile(lvl, (i[1], i[2]))
            # print(np.asarray(tmp_tile).shape)
            # tmp_tile= tensor(tmp_tile).permute(2,0,1)/255
            tmp_tile= get_tile(lvl, i[1], i[2], new_size)
            # print('->', tmp_tile.shape)
            all_img.append(tmp_tile)
        all_img= tensor(np.stack(all_img))

    if tfms is not None:
        all_img= tfms(all_img)
    return all_img

# rename for backward compatibility
get_tile_images= get_tile_tensors

# Cell

def image_list_to_tensors(fnames, tfms= TT.CenterCrop(224)):
    '''
    given a list of image files, load them and apply trasformations and return the tensor

    fnames: image file names
    tfms: image transformations. default is center crop to 224 pixels
    '''
    all_img= []

    for i in tqdm(fnames):
        tmpimg= tensor(Image.open(i)).permute(2,0,1)/255.
        all_img.append(tmpimg)
    all_img= tensor(np.stack(all_img))
    if tfms is not None:
        all_img= tfms(all_img)
    return all_img

# Cell

def get_vocab(learn):
    '''
    given a learner or a list of learners, get the target classes. if a list of learners is provided (e.g. in the case of cross-validation models), the target classes are also checked to ensure consistency.

    learn: a learner or a list of learners

    '''
    if type(learn) is Learner:
        return learn.dls.vocab
    elif type(learn) is list:
        vocab_list= [i.dls.vocab for i in learn]
        if not all([i== vocab_list[0] for i in vocab_list]):
            raise Exception("Error: inconsistent vocab among models!")
        return vocab_list[0]

# Cell

def run_pred_from_images(learn, images, bs= 64, verbose= True):
    '''
    given a learner anda set of images (samples x 3 x 224 x 224), return the prediction scores, class labels, and one-hot encoded classes

    == input:
    learn: CNN model(s). if a list of models is provided, the predictions will be averaged.
    images: image tensors
    bs: batch size

    == return:
    tuple of aw scores, predicted labels, predictions as one-hot encoded matrix, image categories

    == NOTE:
    * Multi-GPU processing is handled when loading the 5-fold cross-validation models
    * TO DO: add option to transforms input images

    '''
    all_pp= []
    n= len(images)

    # GPU numbers
    has_gpu= torch.cuda.is_available()
    num_gpus= torch.cuda.device_count()

    ens_mode= False

    if type(learn) is Learner:
        if verbose:
            print("using single CNN model")
        vocab= learn.dls.vocab
    elif type(learn) is list and (all([type(i) is Learner for i in learn])):
        ens_mode= True
        vocab_list= [i.dls.vocab for i in learn]
        if not all([i== vocab_list[0] for i in vocab_list]):
            raise Exception("Error: inconsistent vocab among models!")

        vocab= learn[0].dls.vocab
        if verbose:
            print("using cross-valitation models: ", len(learn))
    else:
        raise Exception("input model has to be a learner or a list of learners")

    for i in range(0, n, bs):
        tmp_batch= images[i:i+bs]
        # print(i, tmp_batch.shape)

        if has_gpu:
            tmp_batch= tmp_batch.cuda()

        with torch.no_grad():
            if ens_mode:
                all_ens_pp= []
                for m in learn:
                    pp= m.model.eval()(tmp_batch)
                    pp= pp.sigmoid().cpu()
                    all_ens_pp.append(pp)
                all_ens_pp= np.dstack(all_ens_pp)
                # print(all_ens_pp.shape)
                pp= tensor(np.mean(all_ens_pp, axis= 2))
            else:
                pp= learn.model.eval()(tmp_batch)
                pp= pp.sigmoid().cpu()
            all_pp.append(pp)

    pred2= torch.vstack(all_pp)

    #with torch.no_grad():
    #    pred2= learn.model.eval()(images.cuda())
    #    pred2= pred2.sigmoid().cpu()

    pred_class= [list(vocab[i > .5]) for i in pred2]

    return pred2, pred_class, pred2> .5, vocab


# Cell

def run_pred_from_coords(learn, dat_zoom, coords, bs=64, scale= 1, progress= False, verbose= True):
    '''
    given a CNN model, a set of coordinates and a deepzoom object, run CNN model to return the raw scores, predicted classes, and binary predictions

    bs: batch size
    scale: rescale the image tiles if not 1

    '''
    n= len(coords)

    a1= list()
    a2= list()
    a3= list()

    for i in range(0, n, bs):
        tmp_batch= coords[i:i+bs]

        if progress is False:
            print('.', end='')

        tissue_imgs= get_tile_tensors(dat_zoom, tmp_batch, scale= scale, progress= progress)
        tissue_pred= run_pred_from_images(learn, tissue_imgs, bs= bs, verbose= verbose)

        a1.append(tissue_pred[0])
        a2.extend(tissue_pred[1])
        a3.append(tissue_pred[2])

    a1= tensor(np.vstack(a1))
    # a2= np.vstack(a2)
    a3= tensor(np.vstack(a3))
    return a1, a2, a3

# Cell

def show_images_on_grid(img_tensors, pred= None, y= None, max_n= 25, figsize= (12,12)):
    '''
    given image tensors, show them on a 12x12 grid

    pred: predicted classes as a list of lists
    '''
    plt.figure(figsize= figsize)
    nx= math.ceil(math.sqrt(max_n))
    ny= nx

    # max_n= min(nx*ny, max_n)

    if img_tensors.shape[0]> max_n:
        img_tensors= img_tensors[:max_n]

    if (pred is not None) and (y is not None):
        # do diff coloring
        col_comp= True
    else:
        col_comp= False

    n= img_tensors.shape[0]
    for i in range(n):
        plt.subplot(nx,ny,i+1)
        plt.imshow(img_tensors[i].permute(1,2,0))

        plt.xticks([])
        plt.yticks([])
        plt.axis('off')

        if pred is not None:
            if y is not None:
                yi= y[i].split("|")
                if set(pred[i])== set(yi):
                    col= 'g'
                else:
                    col= 'r'
                title= "{}\n{}".format(y[i], "|".join(pred[i]))
            else:
                title= "|".join(pred[i])
                col= 'black'
            title_obj= plt.title(title)
            plt.setp(title_obj, color= col)

    if pred is None:
        plt.subplots_adjust( wspace=0, hspace=0)

# Cell

def run_pred_from_df(learn, df, fcol= 'path', lcol= 'Label_bio_str', bs= 64, show= True, max_n= 25, figsize= (12,12)):
    '''
    given a dataframe or list of file paths, run the CNN model(s) and return predictions. if true labels are given we can plot the images along with true labels on a grid.

    learn: a learner or a list of learners
    fcol: column for file names
    lcol: column for labels
    show: show images, predictions, and true labels on an image grid

    '''

    if type(df)==list:
        fnames= df
        lcol= None
    else:
        fnames= df[fcol].tolist()

    vocab= get_vocab(learn)

    ans= image_list_to_tensors(fnames)
    print('input image sizes:', ans.shape)
    print('classes to predict:', ", ".join(vocab))

    y_real= None
    if lcol is not None:
        y_real= df[lcol].tolist()

    pred= run_pred_from_images(learn, ans, bs= bs)

    if show:
        show_images_on_grid(ans, pred= pred[1], y= y_real, figsize= figsize, max_n= max_n)
    return pred

# Cell

def train_cv5_models(dat_meta_bioset, modelbase, skf, bs= 128, cycles= [20, 20, 30], path= '.', item_tfms= TorchTransform(tfs2), weights= None, wandb_name= None, verbose= True):
    '''
    Train 5 CNN models using 5-fold cross validation given the 5 splits (skf). Here we assume a fixed CNN model (Resnet50 backbone + concat pooling)

    dat_meta_biobase: dataframe with file names and labels
    modelbase: model base name for exporting the three stages of models
    skf: cross-validation splits function
    cycles: number of cycles during fitting the head, head+body (#1), and head+body (#2)
    path: working direcotry
    bs: batch size
    weights: None or `balanced`

    '''

    cnt= 0
    n_gpu=  torch.cuda.device_count()

    cbs= None

    os.environ["WANDB_SILENT"] = "true"
    # os.environ['WANDB_CONSOLE']= 'off'

    if weights is not None:
        weights= weights.lower()
        assert weights=='balanced', "Error: check the name of the class weight method"

    for i, j in skf.split(dat_meta_bioset['File'], dat_meta_bioset['Label_bio_str']):
        tmp_model_file1= '{}1_fold{}'.format(modelbase, cnt)
        tmp_model_file2= '{}2_fold{}'.format(modelbase, cnt)
        tmp_model_file3= '{}3_fold{}'.format(modelbase, cnt)

        # set up training and validation subsets
        df_copy= dat_meta_bioset.copy()
        df_copy['valid']= False
        # df_copy.iloc[j,['valid']]= True
        df_copy.iloc[j, df_copy.columns=='valid'] = True

        print("Cross-validation round #{} train={}, test={}".format(cnt, len(i), len(j)))

        if wandb_name is not None:
            wandb.init(group= wandb_name)
            cbs=[WandbCallback(log_dataset=True, log_model=True), SaveModelCallback()]

        # build dataset = dataloaders
        dls_f= ImageDataLoaders.from_df(df_copy, fn_col= 3, label_col= 5, label_delim= "|", valid_col= 'valid', bs= bs, item_tfms= item_tfms, seed= 123, path= path)

        # train CNN

        if weights == 'balanced':
            cw= calc_class_weights(dls_f)
            if n_gpu>0:
                cw= cw.to('cuda')
            loss_func= BCEWithLogitsLossFlat(pos_weight= cw)
            if verbose:
                print(' class weights: ', cw)
        else:
            loss_func= BCEWithLogitsLossFlat()

        learn_f = cnn_learner(dls_f, resnet50, concat_pool= True, first_bn= False, normalize= False, metrics= [acc_02, acc_05], loss_func= loss_func, cbs= cbs)

        if n_gpu> 1:
            learn_f.model= nn.DataParallel(learn_f.model)

        if True:
            learn_f.fit_one_cycle(cycles[0], 0.01)
            learn_f.save(tmp_model_file1)

            learn_f.unfreeze()
            learn_f.fit_one_cycle(cycles[1], 0.0001)
            learn_f.recorder.plot_loss(with_valid= True)
            learn_f.save(tmp_model_file2)

            learn_f.fit_one_cycle(cycles[2], 0.0001)
            learn_f.save(tmp_model_file3)
        print('final model: ', tmp_model_file3)
        cnt+= 1

        if wandb_name is not None:
            wandb.finish()

# Cell

def get_cv5_models(dat_meta_bioset, skf, model_base, bs= 128, path= '.', gpu= True, load= True, item_tfms= TorchTransform(tfs2)):
    '''
    load the trained models from 5 fold cross-valitation specified by skf and return the models and corresponding datasets

    NOTE: currently we use the fastai cnn_learner so the learner is tied to the original dataloaders to enable functions for performance evaluation

    TO DO: swap in dataloaders template or add option to load only the models

    dat_meta_bioset: dataframe containing file, label, and validation columns
    skf: 5-fold cross-validation splits
    model_base: basename of the pre-trained cross-validation model
    bs: batch size
    gpu: use GPU(s) if available
    item_tfms: item level transformation

    '''
    cnt= 0

    # all_preds= []
    # all_y= []
    df_val= pd.DataFrame()

    all_models= []

    gpu_count= torch.cuda.device_count()

    if gpu is False:
        default_device(use_cuda= False)
    else:
        default_device(use_cuda= None)

    for i, j in skf.split(dat_meta_bioset['File'], dat_meta_bioset['Label_bio_str']):
        tmp_model_file3= '{}{}'.format(model_base, cnt)

        df_copy= dat_meta_bioset.copy()
        df_copy['valid']= False
        df_copy.iloc[j, df_copy.columns=='valid'] = True
        df_copy['fold']= cnt

        print("loading cross-validation model #{} train={}, test={}".format(cnt, len(i), len(j)))

        # build dataset
        dls_fr= ImageDataLoaders.from_df(df_copy, fn_col= 3, label_col= 5, label_delim= "|", valid_col= 'valid', bs= bs, item_tfms= item_tfms, seed= 123, path= path)

        # load trained CNN - fr= fold reloaded
        learn_fr = cnn_learner(dls_fr, resnet50, concat_pool= True, first_bn= False, normalize= False, metrics= [acc_02, acc_05])

        if load:
            print("loading model: {}".format(tmp_model_file3))

            if gpu and gpu_count > 0:
                learn_fr.load(tmp_model_file3, device= torch.device('cuda'))
            else:
                learn_fr.load(tmp_model_file3, device= torch.device('cpu'))

            if gpu_count > 1:
                learn_fr.model= nn.DataParallel(learn_fr.model)

            if gpu and gpu_count > 0:
                learn_fr.model= learn_fr.model.cuda()

        all_models.append(learn_fr)

        #out_fr= learn_fr.get_preds(with_decoded= False)
        #all_preds.append(out_fr[0].cpu())
        #all_y.append(out_fr[1].cpu())

        df_val= df_val.append(df_copy.loc[df_copy['valid']])

        cnt+= 1

    return all_models, df_val

# Cell

def pred_cv5(dat_meta_bioset, skf, model_base, bs= 128):
    '''
    given CNN models from 5-fold cross-valitation , predict on the respective test sets and return:
    predictions, true labels, dataframe of the validation sets, vocabulary (target classes)

    == input
    dat_meta_bioset: dataframe of meta data containing `File` and `Label_bio_str` columns
    skf: sklearn's cross validation splits
    model_base: string of base model name
    bs: batch size

    '''
    cnt= 0

    all_preds= []
    all_y= []
    df_val= pd.DataFrame()

    gpu_count= torch.cuda.device_count()

    for i, j in skf.split(dat_meta_bioset['File'], dat_meta_bioset['Label_bio_str']):
        # tmp_model_file1= 'model_MLC_export5_bio_c10_lr0.01_stage1_fold{}'.format(cnt)
        # tmp_model_file2= 'model_MLC_export5_bio_c10_lr0.01_stage2_fold{}'.format(cnt)
        tmp_model_file3= '{}{}'.format(model_base, cnt)

        df_copy= dat_meta_bioset.copy()
        df_copy['valid']= False
        df_copy.iloc[j, df_copy.columns=='valid'] = True
        df_copy['fold']= cnt

        print("loading cross-validation model #{} train={}, test={}".format(cnt, len(i), len(j)))

        # build dataset
        dls_fr= ImageDataLoaders.from_df(df_copy, fn_col= 3, label_col= 5, label_delim= "|", valid_col= 'valid', bs= bs, item_tfms= TorchTransform(tfs2), seed= 123)

        # load trained CNN - fr= fold reloaded
        learn_fr = cnn_learner(dls_fr, resnet50, concat_pool= True, first_bn= False, normalize= False, metrics= [acc_02, acc_05])

        print("loading model: {}".format(tmp_model_file3))

        if gpu_count > 0:
            learn_fr.load(tmp_model_file3, device= torch.device('cuda'))
            learn_fr.model= learn_fr.model.cuda()
        else:
            learn_fr.load(tmp_model_file3, device= torch.device('cpu'))

        out_fr= learn_fr.get_preds(with_decoded= False)
        all_preds.append(out_fr[0].cpu())
        all_y.append(out_fr[1].cpu())

        df_val= df_val.append(df_copy.loc[df_copy['valid']])

        cnt+= 1

    all_preds= torch.vstack(all_preds)
    all_y= torch.vstack(all_y)
    return all_preds, all_y, df_val, dls_fr.vocab

# Cell

# plot multilabel ROC
def plot_multi_roc(y_true, y_score, cls, plot_cls= [], sub_idx= None, figsize= (15,15), addAUC= True, newplot= True):
    '''
    plot ROC curves for a multi-label classifier

    y_true: sample x target matrix
    y_score: sample x feature matrix
    cls: class names along the feature/target dimension

    sub_idx: list of numerical or binary indices for samples to use for analyssis

    '''

    if len(plot_cls)==0:
        plot_cls= cls

    if newplot:
        plt.figure(figsize= figsize)

    if sub_idx is not None:
        if type(sub_idx) is pd.core.series.Series:
            sub_idx = sub_idx.tolist()

        y_true= y_true[sub_idx, :]
        y_score= y_score[sub_idx, :]

    for i in range(y_true.shape[1]):
        if cls[i] not in plot_cls:
            continue
        roc= sklearn.metrics.roc_curve(y_true[:,i], y_score[:,i])
        auc_score= sklearn.metrics.auc(roc[0], roc[1])

        if addAUC:
            plt.plot(roc[0], roc[1], label= "{} AUC= {:.2f}".format(cls[i], auc_score))
        else:
            plt.plot(roc[0], roc[1], label= "{}".format(cls[i]))

    plt.xlim([0.0, 1.0])
    plt.plot([0, 1], [0, 1], 'k--', lw=2)
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('ROC based on 5 fold cross-validation')
    plt.legend(loc="lower right")

# Cell

def plot_merged_preds(ys, preds, idx1, idx2, title='', sub_idx= None, newplot= True, addAUC= True, figsize= (12,12)):
    '''

    '''
    if sub_idx is not None:
        if type(sub_idx) is pd.core.series.Series:
            sub_idx = sub_idx.tolist()

        ys= ys[sub_idx, :]
        preds= preds[sub_idx, :]

    roc= sklearn.metrics.roc_curve((ys[:,idx1] == 1) | (ys[:,idx2] ==1), (preds[:,idx1] + preds[:,idx2]))

    auc_score= sklearn.metrics.auc(roc[0], roc[1])

    if newplot:
        plt.figure(figsize= figsize)

    if addAUC:
        plt.plot(roc[0], roc[1], label= "{} AUC= {:.2f}".format(title, auc_score))
    else:
        plt.plot(roc[0], roc[1], label= "{}".format(title))

    plt.xlim([0.0, 1.0])
    plt.plot([0, 1], [0, 1], 'k--', lw=2)
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.legend(loc= 'lower right')
    plt.title(title)

# Cell

# accuracy in 5-fold cross validation
import sklearn

def get_performance(out_val, threshold= .5, only_annotated= False, verbose= False):
    '''
    return accuracy, AUC, and precision in 5-fold cross validation based on the specified threshold

    == input:
    out_val: output tuple from `pred_cv5()`
    threshold: predictions above this level are deemed positive
    only_annotated: use only manually annotated tags, i.e. no pseudo labels

    '''
    df_perf= pd.DataFrame()

    for i in range(len(out_val[3])):
        if only_annotated:
            idx_annotated= out_val[2]['annotated']== 'Yes'
        else:
            idx_annotated= out_val[2]['annotated'] !=''

        # print("#{} annotated= {} df length= {}".format(i, idx_annotated.sum(), len(out_val[1][:,i])))
        idx_annotated= idx_annotated.tolist()

        # s= np.sum(np.asarray((all_preds[:, i] != all_y[:,i]), dtype= int))
        s= sklearn.metrics.accuracy_score(out_val[1][idx_annotated, i], out_val[0][idx_annotated, i] > threshold)
        roc= sklearn.metrics.roc_curve(out_val[1][idx_annotated, i], out_val[0][idx_annotated, i])

        auc_score= sklearn.metrics.auc(roc[0], roc[1])

        prc= sklearn.metrics.precision_score(out_val[1][idx_annotated, i], out_val[0][idx_annotated, i] > threshold)

        n= np.asarray(out_val[1][idx_annotated, i]).sum().astype(int)

        df_perf= df_perf.append(pd.DataFrame({"Category": [out_val[3][i]],
                                                 "n": [n],
                                                "Accuracy": [s],
                                             "AUC": [auc_score],
                                             "Precision": [prc]}))
        if verbose:
            print("#{} -> {} -> n= {} -> acc= {:.2f}, auc= {:.2f}, precision= {:.2f}".format(i, out_val[3][i], n, s, auc_score, prc))
    return df_perf

# Cell

import math

def plot_multi_prec_recall(y_true, y_score, cls, figsize= (20,12), use_threshold= False, sub_idx= None, step= 5, nrows= 2):
    '''
    plot precision-recall and precision-threshold curves per image category. if subset indices (`sub_idx`) are provided, the plot will be based on the subset

    == input:
    sub_idx: list of numerical or binary indices for samples to use for analyssis
    '''
# cl_idx= 0

    plt.figure(figsize= figsize)

    num= y_true.shape[1]

    if sub_idx is not None:
        if type(sub_idx) is pd.core.series.Series:
            sub_idx = sub_idx.tolist()

        y_true= y_true[sub_idx, :]
        y_score= y_score[sub_idx, :]

    for cl_idx in range(y_true.shape[1]):
        plt.subplot(nrows, math.ceil(num/nrows), cl_idx+1)
        plt.title(cls[cl_idx])

        precision, recall, thresholds = precision_recall_curve(y_true[:,cl_idx], y_score[:, cl_idx])

        thresholds= thresholds.tolist()
        thresholds.append(thresholds[-1])
        thresholds= np.array(thresholds)

        tp= range(0, len(precision), step)
        if use_threshold:
            tp= tp[1:] # remove the artificially set precision of 1
            plt.plot(thresholds[::-1][tp], precision[::-1][tp])
            plt.xlabel('Threshold')
        else:
            plt.plot(recall[::-1][tp], precision[::-1][tp])
            plt.xlabel('Recall')
        plt.ylabel('Precision')
        plt.ylim([0.0, 1.05])
        plt.xlim([0.0, 1.05])

# Cell

def plot_cluster_scores(dat, cid, show_images= True, seed= 123):
    '''
    given a prediction score dataframe and a cluster id, plot the distribution of prediction scores
    '''

    tmpdat= dat[ dat['cluster']== cid]
    max_n= 9

    n_images= tmpdat.shape[0]
    n_train_val= tmpdat['train_val'].sum()
    pct_labeled= n_train_val/n_images*100.

    random.seed(seed)
    rnd_list= random.sample(range(n_images), max_n)

    sc= tmpdat['Score']
    sc= np.vstack(np.asarray(sc))

    n_lasses= sc.shape[1]

    # make plots
    if show_images:
        tmp_imgs= image_list_to_tensors(tmpdat['File'].tolist())
        pred_labels= tmpdat['Prediction'].tolist()

        plt.figure(figsize= (12,12))

        for i in range(max_n- 1):
            plt.subplot(3,3,i+1)
            plt.imshow(tmp_imgs[ rnd_list[i] ].permute(1,2,0))
            plt.title("#{} {}".format(rnd_list[i], "|".join(pred_labels[ rnd_list[i] ])))
            plt.xticks([])
            plt.yticks([])
            plt.axis('off')

        plt.subplot(3,3, max_n)
        # plot_cluster_scores(dat_pred_full, 100)


    plt.boxplot(sc, positions= range(n_lasses), showfliers= False);

    plt.ylim(-.1, 1.1)
    plt.ylabel('Prediction score')

    plt.title(f'Cluster {cid}, in training/test={pct_labeled:.2f}% ')

    # print(learn.dls.vocab)
    plt.xticks(range(n_lasses), labels= learn.dls.vocab, rotation= 45, horizontalalignment='right');



# Cell

def show_images_for_predicted_label(dat, cl, max_n= 16, figsize= (12,12)):
    '''
    output the images predicted to be the specified class. images are outside of the original training/test set

    '''

    if cl=='':
        # no predictions made for those images
        list_class= dat[(dat['Prediction'].apply(lambda x:len(x)==0)) &  (dat['train_val']==False)]['File'].tolist()
    else:
        list_class= dat[(dat['Prediction'].apply(lambda x: cl in x)) & (dat['train_val']==False)]['File'].tolist()
    print('number of images not in training/test sets:', len(list_class))

    assert len(list_class)>0, 'No images found'

    random.seed(123)

    if len(list_class)> max_n:
        list_class_sub= random.sample(list_class, max_n)
    else:
        list_class_sub= list_class

    img_class= image_list_to_tensors(list_class_sub)

    show_images_on_grid(img_class, max_n= max_n, figsize= figsize)
    plt.suptitle(cl, verticalalignment='center')
    return list_class, list_class_sub

# Cell

def make_masked_channels(preds, coords, cl, xmax, ymax, show= True, bar= False, useGlass= False, verbose= False):
    '''

    use the predicted labels to create a segmentation mask for the specified class `cl`. If useGlass is specified, the Glass/Blank prediction is used to mark non-glass areas as 0.

    TO DO: make masked channels based on raw prediction scores

    ==
    preds:
    coords:
    cl: class name (str)
    xmax, ymax:
    bar: show color scale
    useGlass: use glass/blank prediction to mark tissue areas

    '''
    masked_img= np.zeros((xmax, ymax))

    cnt_nbg= 0.  # non-background tile count
    cnt_cl= 0.   # image class tile count

    for i in range(len(coords)):
        cds= coords[i]
        tmp_labels= preds[1][i]

        if useGlass:
            col= 0
            if 'Glass/Blank' not in tmp_labels:
                col= 0.5
                cnt_nbg+= 1
        else:
            col= 0.5
            cnt_nbg+= 1

        if cl in tmp_labels:
            col= 1
            cnt_cl+= 1

        masked_img[cds[1], cds[2]]= col

    if useGlass and cl=='Glass/Blank':
        cnt_cl= None
        title= cl
    else:
        title= "{} ({:.2f}%)".format(cl, cnt_cl/cnt_nbg*100.)

    if verbose:
        print(cl, 'class tiles:', cnt_cl)
        print('non-background tiles:', cnt_nbg)

    if show:
        # plt.figure(figsize=(12,12))
        plt.imshow(masked_img.transpose(), vmax= 1, vmin= 0,  interpolation= 'none')
        plt.title(title)
        if bar:
            plt.colorbar()
    return masked_img

# Cell

def plot_seg_maps(slide_preds, coord, cls, xmax, ymax, nrows= 2, useGlass= False, verbose= True):
    '''
    show on a grid segmentation heatmaps for the specified image classes

    ==
    slide_preds:
    coords:
    cls: image classes to show
    nrows: number of rows
    useGlass: use CNN prediction to determine if a tile is background. if false, use pre-computed tile coordinates
    '''
    nx= math.ceil( len(cls) / nrows)

    for i in range(len(cls)):
        plt.subplot(nrows, nx, i+1)
        make_masked_channels(slide_preds, coord, cls[i], xmax, ymax, show= True, useGlass= useGlass, verbose= verbose)
        plt.xticks([])
        plt.yticks([])

# Cell

def img_to_segmaps(learn, img_file, coord_file= None, bs= 128, cls=  ['Tumor', 'Stroma', 'Connective tissue', 'Lymphocytes', 'Normal','Glass/Blank', 'RBCs'], progress= False, nrows= 1, tile_size= 512, useGlass= False, verbose= False, figsize= (25,15)):
    '''

    Given a CNN model and an image file, predict the morphological classes along with th tile coordinates. if coord_file is not provided, use all tiles at the highest magnification level. note the images from the last row/column are not processed as they are usually cut-off causing inconsistent dimensions

    Updated 12/2/2021: now return a dict instead of a tuple of slide_preds, coord, (xmax, ymax)

    NOTE: currently we use the layer with the highest magnification for prediction - we might add an option to determine the layer based on resolution or magnificaiton level.
    TODO: if mpp==.5 -> use last layer and set tilesize = half -> extract tile -> resize to full tilesize

    ==
    learn: CNN learner
    img_file: H&E image file
    coord_file: numpy file of tile coordinates
    bs: batch size
    cls: image classes to plot
    tile_size: image tile size (pixels) - npy coordinate files were generated using 512px tiles
    useGlass: use predicted glass category instead of the Otsu method to remove background
    verbose:
    figsize:

    '''

    # img_name= 'data/GEN1046/HE_converted/HE_12380001C0001X.mrxs'
    # np_name= 'output/split_HE_conv/m20x_w512px/HE_12380001C0001X/coords.pkl'

    dat_img= openslide.OpenSlide(img_file)

    # check MPP
    mpp= float(dat_img.properties[openslide.PROPERTY_NAME_MPP_X])
    mag= float(dat_img.properties[openslide.PROPERTY_NAME_OBJECTIVE_POWER])

    scale= 1
    if (abs(mpp - 0.5) < .1):
        print("# Warning: MPP= {} magnification= {}".format(mpp, mag))
        scale= 2
        tile_size= tile_size/2
    else:
        assert (abs(mpp - 0.24) < .1), "Error. MPP ({}) is different from training set".format(mpp)

    dat_zoom= openslide.deepzoom.DeepZoomGenerator(dat_img, tile_size= tile_size, overlap= 0)
    xmax, ymax= dat_zoom.level_tiles[-1]

    if coord_file is not None:
        coord= pkl.load(open(coord_file, 'rb'))['All_tiles']
    else:
        # use the highest magnification level by default
        lvl= dat_zoom.level_count- 1
        coord= [(lvl, i, j) for i in range(xmax-1) for j in range(ymax-1)]

    if verbose:
        print("mpp= {}".format(mpp))
        print('xmax= {}, ymax= {}'.format(xmax, ymax))
        print('total tiles: {}'.format(len(coord)))

    # NOTE: need to rescale image tiles
    slide_preds= run_pred_from_coords(learn, dat_zoom, coord, progress= progress, bs= bs, scale= scale, verbose= verbose)

    plt.figure(figsize= figsize)
    plot_seg_maps(slide_preds, coord, cls, xmax, ymax, nrows= nrows, useGlass= useGlass, verbose= verbose)

    # return slide_preds, coord, (xmax, ymax)
    return {"slide_preds": slide_preds,
            "coord": coord,
            "max_xy": (xmax, ymax)}


# Cell

from tqdm import tqdm

def summarize_image_specs(fname):
    '''
    make a summary table of image specs for the input file(s)

    fname: a file name or a list of file names
    '''

    if type(fname) is list:
        out= pd.DataFrame()
        for i in tqdm(fname):
            out= out.append(summarize_image_specs(i))
        return out

    dat_img= openslide.OpenSlide(fname)
    out= pd.DataFrame({"File": [fname],
                       "Sample": [os.path.basename(fname).replace(".svs",'').replace(".mrxs",'')],
                        "Vendor": [dat_img.properties[openslide.PROPERTY_NAME_VENDOR]],
                      "Magnification": [int(dat_img.properties[openslide.PROPERTY_NAME_OBJECTIVE_POWER])],
                      "MPP": [float(dat_img.properties[openslide.PROPERTY_NAME_MPP_X])]
                      })
    return out

# Cell

import pickle as pkl

def make_segmaps(learn, files, outdir, useGlass, cls= None, bs= 1024, tile_size= 512, figsize= (30,15), nrows= 1):
    '''
    create segmentation heatmaps for a H&E image. output coordinates are stored in outdir/sample.pkl

    learn: CNN model
    files: a list of H&E images
    outdir: output directory path
    useGlass: if ture, infer tissue tiles using the CNN, otherwise use pre-computed coordinates from .npy (`Otsu`-based foreground separation)
    cls: image categories to predict. if None, use all classes from the CNN learner
    tile_size: needs to be 512 if using predefined coordinates (useGlass= False). otherwise use something bigger than 224 and the classifier will center crop to 224px for the CNN
    '''
    # output/multi_preds

    if os.path.exists(outdir) is False:
        print('creating ', outdir)
        os.makedirs(outdir, exist_ok= True)

    print("using {} CNN models".format(len(learn)))

    if cls is None:
        cls= get_vocab(learn)

    for i in files:
        samp= os.path.basename(i).replace('.mrxs','')

        np_file= 'output/split_HE_conv/m20x_w512px/{}/coords.pkl'.format(samp)
        outfile= '{}/{}.pkl'.format(outdir, samp)
        pdffile= '{}/{}_plots.pdf'.format(outdir, samp)

        print('\nprocessing',i)

        if os.path.exists(outfile):
            print("{} output exists... skipped...".format(samp))
            continue
        try:
            with PdfPages(pdffile) as pdf:

                if useGlass:
                    seg_out= img_to_segmaps(learn, i, bs= bs, cls= cls, progress= False, useGlass= True, tile_size= tile_size, figsize= figsize, verbose= False, nrows= nrows)
                else:
                    seg_out= img_to_segmaps(learn, i, np_file, bs= bs, cls= cls, progress= False, useGlass= False, tile_size= tile_size, figsize= figsize, verbose= False, nrows= nrows)

                plt.suptitle(samp)
                pdf.savefig()
                plt.close()

            pkl.dump(seg_out, open(outfile, 'wb'))
        except Exception as inst:
            print("Error processing", i)
            print(inst)

# Cell

from shapely.geometry import Polygon, MultiPolygon
from skimage import measure

def overlay_image_segments(img_file, pred_file, out_file= None, cl='Tumor', maxsize= 5000, minseg= 10, lwd= 2.5, linestyle='solid', color= 'red', figsize= (25,25), simplify= True, verbose= True):
    '''
    overlay image annotations as contours on the thumbnail of an H&E image.

    NOTE: since we are using `measure.find_contours`, we can only show the external contours.

    --
    img_file: H&E image file
    pred_file: file name or prediction output object;
               if file name, point to the predicted annotations in 'pkl' format;
               otherwise predictions are the output of `run_pred_from_coords` or `img_to_segmaps` in the format [preds, coords, (xmax, ymax)]
    out_file: output image file
    cl: predicted image class to overlay
    maxsize: max number of pixels on x or y in the output figure

    -- segment controls --
    minseg: plot segments with length >= minseg
    lwd: line width
    linestyle: default = solid
    color: line color
    simplify: smooth the contour of the segments

    -- other --
    figsize: figure size

    -- return --
    contours:
    sf: scaling factor
    '''
    # img_name= 'data/GEN1046/HE_converted/HE_12380001C0001X.mrxs'

    # sf= 2 # scaling factor
    dpi= 200

    dat_img= openslide.OpenSlide(img_file)

    if type(pred_file) is str:
        preds= pkl.load(open(pred_file, 'rb'))
    else:
        preds= pred_file

    m1= make_masked_channels(preds[0], preds[1], cl, preds[2][0], preds[2][1], useGlass= True, show= False)
    nx, ny= m1.shape

    if verbose:
        print('full image size: ', dat_img.dimensions)
        print('heatmap size: ', m1.shape)

    sf= maxsize/max(nx,ny)
    if verbose: print('sf: ', sf)

    med_img= dat_img.get_thumbnail((nx* sf, ny* sf))
    img_mtx=  np.asarray(med_img)
    if verbose: print('thumbnail size: ', img_mtx.shape)

    # increase resolution of contours
    sc= 2
    m1= np.kron(m1, np.ones((2,2)))
    contours = measure.find_contours(m1, fully_connected= 'high')

    if out_file is None:
        plt.figure(figsize= figsize)
    else:
        plt.figure(figsize= (img_mtx.shape[0]/dpi, img_mtx.shape[1]/dpi))

    plt.imshow(img_mtx)
    for i in range(len(contours)):
        if len(contours[i])< minseg:
            continue
        if simplify:
            poly = Polygon(contours[i]/sc)
            poly = poly.simplify(1.0, preserve_topology=True)
            # polygons.append(poly)
            segmentation = np.array(poly.exterior.coords)
            xs, ys = zip(*(segmentation -0.25)*sf)  # old: + .5
        else:
            xs, ys = zip(*(contours[i]/sc - 0.25)*sf) # old: + .5
        plt.plot(xs,ys, color= color, linewidth= lwd, linestyle= linestyle)

    if out_file is not None:
        plt.savefig(out_file, dpi= dpi)

    return contours, sf

# Cell

# plt.figure(figsize= (20,20))
# plt.imshow(m1.transpose())
from shapely.geometry import Polygon, MultiPolygon

def plot_contours_holes(contours, color, lwd= 5, minlen= 5, simplify= True, sf= 1, show= True):
    '''
    plog contour segments defined in upolygon format

    == input:
    contours: segments in upolygon format
    color: line color
    lwd: line width
    minlen: min length of contour to plot
    sf: scaling factor
    show: plot segments on a figure

    == return:
    full set of (x, y) coordinates as a list
    '''
    full_xy= []
    for i in range(len(contours)):
        if len(contours[i])< minlen*2:
            continue

        tmpc= [(contours[i][x], contours[i][x+1]) for x in range(0, len(contours[i]), 2)]

        if simplify:
            poly = Polygon(tmpc)
            poly = poly.simplify(1.0, preserve_topology=True)
            # polygons.append(poly)
            segmentation = np.array(poly.exterior.coords)
            xs, ys = zip(*(segmentation)*sf)
        else:
            xs, ys = zip(* tmpc)

        if show:
            plt.plot(xs,ys, color= color, linewidth= lwd)

        full_xy.append((xs, ys))
    return full_xy

# Cell

import upolygon

def add_segment_contours(segmap, minlen= 50, lwd= 3, simplify= False, figsize= (15,15), show= True):
    '''
    add segment contours to a heatmap

    == input:
    segmap: segmentation as a binary matrix
    minlen: min length of contour to plot
    lwd: line width
    simplify: simplify the contour
    figsize:
    show: if true, show the heatmap with contours overlaid. otherwise simply return the internal and external contour coordinates

    == return:
    internal coordinates (negative ROA)
    external coordinates
    '''
    m1= segmap.transpose()

    _labels, external_paths, internal_paths = upolygon. find_contours(m1==1)

    if show:
        plt.figure(figsize= figsize)
        plt.imshow(m1)

    # should be something like
    # halo_xml= halo_build()
    # halo_xml.add_region('', external_xys, ROA=-1)
    xy_external= plot_contours_holes(external_paths, 'red', lwd, minlen, simplify= simplify, show= show)
    xy_internal= plot_contours_holes(internal_paths, 'white', lwd, minlen, simplify= simplify, show= show)
    return xy_internal, xy_external

# Cell


# should be something like
# halo_xml= halo_build()
# halo_xml.add_region('', external_xys, ROA=-1)

class halo_xml():
    '''
    here we do one set of regions per annotation id

    '''
    all_children= {} # annotation level
    all_regions= {}

    def __init__(self, format= 'halo', mpp= 0.243094):
        if format == 'halo':
            self.top = Element('Annotations')
        else:
            self.top= Element('Annotations', {"MicronsPerPixel": str(mpp)})
        self.format= format

    def add_annotation(self, id= None, color= '65535'):
        if id is None:
            id= str(len(self.all_children)+1) # 1-indexing
        if type(id) is not str:
            id= str(id)

        # print('id= ', id)
        if self.format=='halo':
            self.all_children[id] = SubElement(self.top, 'Annotation', {"Id": id, "Name": id, "Visible": "True", "LineColor": str(color)})
        else:
            # concentriq
            self.all_children[id] = SubElement(self.top, 'Annotation', {"Id": id, "Name": id, "ReadOnly": "0",
                                                                        "LineColorReadOnly": "0",
                                                                        "Incremental": "0",
                                                                        "Type": "4",
                                                                        "Selected": "0",
                                                                        "MarkupImagePath": "", "MarcoName": "",
                                                                        "Visible": "True", "LineColor": "2585033"})
            attr= SubElement(self.all_children[id] , "Attributes")

        self.all_regions[id]= SubElement(self.all_children[id], "Regions")

        if format!= 'halo':
            regions_attr= SubElement(self.all_regions[id], "RegionAttributeHeaders")

    def add_region(self, parent, vertices, negative= 0, sf= 1, offset_x=0, offset_y= 0):
        '''
        add one region per set of vertices. the final coordinates will be scaled (`sf`) and offset (`offset_x`, `offset_y`) to match the raw image dimensions.

        parent: annotation id
        vertices: polygon coordinates
        negative: negative ROA indicator (0/1)
        sf: scaling factor to convert heatmap coordinates to full image size
        offset_x: offset x coordinates by this amount
        offset_y: offset y coordinates by this amount
        '''

        # print('keys: ', self.all_regions.keys())
        if parent not in self.all_regions:
            print('adding parent=', parent)
            self.add_annotation(parent)

        for i in range(len(vertices)):
            v_str= 'V'
            if self.format=='halo':
                reg = SubElement(self.all_regions[parent], 'Region', {"Type": "Polygon", 'HasEndcaps': '0', "NegativeROA": str(negative)})
            else:
                reg = SubElement(self.all_regions[parent], 'Region', {"Id": str(i),
                                                                      "Selected": "0",
                                                                      "ImageLocation":"", "ImageFocus":"-1",
                                                                      "InputRegionId": str(i),
                                                                      "Analyze": "0",
                                                                      "DisplayId": str(i),
                                                                      "Type": "0", "NegativeROA": str(negative)})

                reg_attr= SubElement(reg, 'Attributes')
                v_str= 'Vertex'

            # reg_attr= SubElement(reg, 'Attributes', {"Id":str(i), 'Selected': "0", 'NegativeROA': str(negative)})

            vs= SubElement(reg, "Vertices")
            for j in range(len(vertices[i][0])):
                tx= int(vertices[i][0][j] * sf + offset_x)
                ty= int(vertices[i][1][j] * sf + offset_y)
                v= SubElement(vs, v_str, {"X": str(tx), "Y": str(ty), "Z": "0"})

            comm= SubElement(reg, "Comments")
    def __repr__(self):
        return prettify(self.top)

    def write(self, fname):
        mydata = prettify(self.top)
        myfile = open(fname, "w")
        myfile.write(mydata)


# Cell

# some xml tests based on https://pymotw.com/2/xml/etree/ElementTree/create.html

from xml.etree.ElementTree import Element, SubElement, Comment, tostring
from xml.etree import ElementTree
from xml.dom import minidom

def prettify(elem):
    """Return a pretty-printed XML string for the Element.
    """
    rough_string = ElementTree.tostring(elem, 'utf-8')
    reparsed = minidom.parseString(rough_string)
    return reparsed.toprettyxml(indent="  ")

def xml_build():
    top = Element('top')

    comment = Comment('Generated for PyMOTW')
    top.append(comment)

    child = SubElement(top, 'child')
    child.text = 'This child contains text.'
    for i in range(5):
        c2 = SubElement(child, 'c2')
        c2.text= "subitem {}".format(i)

    child_with_tail = SubElement(top, 'child_with_tail')
    child_with_tail.text = 'This child has regular text.'
    child_with_tail.tail = 'And "tail" text.'

    child_with_entity_ref = SubElement(top, 'child_with_entity_ref')
    child_with_entity_ref.text = 'This & that'

    # print (tostring(top))
    return top



# Cell

def export_halo_xml_from_img(img_name, slide_preds, coords, outfile, cl= 'Tumor', minlen= 1500, simplify= False, tile_size= 512, show= False, verbose= True):
    '''
    given an input H&E image, class predictions, and coordinates, export annotations in Halo format

    NOTE: this was originally written for 20x images at 0.25um resolution

    == input:
    img_name: svs or mrxs file
    slide_preds: predictions made by `run_pred_from_coords` or `img_to_segmaps`. if by `img_to_segmaps`, the pickle file contains a tuple of slide_preds, coord, (xmax, ymax)
    coords: coordinates given by split image workflow `split_MRXS_image.py` or `img_to_segmaps`
    outfile: output Halo file name (`*.annotations`)
    cl: image class
    minlen:
    simplify:
    tile_size: tile size corresponding to what's used for training the model at 20x and 0.25um. the tile size will be automatically adjusted based on the query image's resolution and magnificaiton level
    show:
    verbose:

    '''

    if verbose: print('loading', img_name)

    dat_img= openslide.OpenSlide(img_name)
    mpp= float(dat_img.properties['openslide.mpp-x'])
    mag= float(dat_img.properties[openslide.PROPERTY_NAME_OBJECTIVE_POWER])

    try:
        offx= -int(dat_img.properties[openslide.PROPERTY_NAME_BOUNDS_X])
        offy= -int(dat_img.properties[openslide.PROPERTY_NAME_BOUNDS_Y])
    except Exception as inst:
        print(inst)
        print("WARNING: no coordinate offsets available. using (0,0)")
        offx= 0
        offy= 0

    scale= 1
    if (abs(mpp - 0.5) < .1):
        print("# Warning: MPP= {} magnification= {}".format(mpp, mag))
        scale= 2
        tile_size= tile_size/ scale
    else:
        assert (abs(mpp - 0.24) < .1), "Error. MPP ({}) is different from training set".format(mpp)

    dat_zoom= openslide.deepzoom.DeepZoomGenerator(dat_img, tile_size= tile_size, overlap= 0)
    xmax, ymax= dat_zoom.level_tiles[-1]

    if verbose:
        print("magnification:", mag)
        print("mpp:", mpp)
        print('offset x:', offx)
        print('offset y:', offy)
        print('minlen:', minlen)
        print('tile size:', tile_size)
        print("scale:", scale)

    if cl is str:
        cl= [cl]

    halo_test= halo_xml()

    for i in cl:
        if verbose:
            print("processing", i)

        m1= make_masked_channels(slide_preds, coords, cl=i, xmax= xmax, ymax= ymax, show= False)

        xy1, xy2= add_segment_contours(m1, minlen= minlen, simplify= simplify, show= show);

        halo_test.add_annotation(i)
        halo_test.add_region(i, xy2, sf= tile_size, offset_y= offy, offset_x= offx)

        neg_name= "Negative_{}".format(i)
        halo_test.add_annotation(neg_name, color= '255')
        halo_test.add_region(neg_name, xy1, sf= tile_size , offset_y= offy, offset_x= offx, negative= 1) # white border for negative ROA

    print('saving to', outfile)
    halo_test.write(outfile)